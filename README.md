# ğŸµ Music Transformer: Generating Music with Long-Term Structure

This repository is a reimplementation of the research paper:  
ğŸ“„ [Music Transformer: Generating Music with Long-Term Structure](https://arxiv.org/abs/1809.04281) by Huang et al., 2018.

> This model leverages the Transformer architecture with a novel **relative positional encoding** mechanism to generate coherent and structured musical sequences over long time spans.

---

## ğŸš€ Features

- Transformer-based architecture optimized for music generation.
- Support for **relative positional self-attention** for capturing long-term dependencies.
- Trained on MIDI-based datasets (e.g., MAESTRO, JSB Chorales).
- Support for both training and inference.
- Generates symbolic music in MIDI format.

---

## ğŸ§  Paper Contributions Summary

- Proposed a **relative attention mechanism** to overcome limitations of absolute position encodings for musical data.
- Demonstrated the model's capability to generate longer and more musically coherent sequences than baseline models.
- Introduced **event-based representation** of MIDI music for efficient tokenization.

---

## ğŸ—‚ï¸ Project Structure

```bashmusic-transformer/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ jsb_chorales/
â”‚   â””â”€â”€ piano_midi/
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ midi_parser.py
â”‚   â”œâ”€â”€ chorale_converter.py
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ transformer.py
â”‚   â”œâ”€â”€ relative_attention.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_baseline.py
â”‚   â”œâ”€â”€ train_relative.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval_nll.py
â”‚   â”œâ”€â”€ generate_music.py
â”‚   â””â”€â”€ visualize_attention.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ generated/
â”‚   â”œâ”€â”€ logs/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

````

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/music-transformer.git
cd music-transformer
pip install -r requirements.txt
```

**Main dependencies:**

* `torch`
* `pretty_midi`
* `numpy`
* `tqdm`
* `pytorch-lightning` *(optional)*

---
## Week 1

- ğŸ“– **Read the Paper Thoroughly**  
  Gain a complete understanding of the Music Transformer paper. Focus on:
  - The motivation behind using relative attention
  - Data representation methods for JSB Chorales and Piano-e-Competition
  - Transformer architecture modifications
  - Evaluation methods and sample generation

- ğŸ“¦ **Dataset Collection**  
  - Download the [JSB Chorales dataset](https://github.com/czhuang/JSB-Chorales-dataset)
  - Download the MIDI files for the [Piano-e-Competition dataset](http://www.piano-e-competition.com/)

- ğŸ§¹ **Prepare for Data Preprocessing**  
  - Study the event-based encoding scheme used in the paper
  - Understand how sustain pedal, timing, and velocity are handled in MIDI preprocessing
  - Plan the pipeline for converting raw MIDI to token sequences



## ğŸ§  Credits

* Original authors: [Anna Huang et al.](https://magenta.tensorflow.org/music-transformer)
* Reimplementation by: [Your Name](https://github.com/yourusername)
* Inspired by: Magenta's official implementation and community forks

---

## ğŸ“¨ Contact

For queries, issues, or suggestions:
ğŸ“§ [lakshinkhurana@gmail.com](mailto:lakshinkhurana@gmail.com)
ğŸ‘œ LinkedIn: [@yourhandle](https://twitter.com/yourhandle)

---

## ğŸª„ License

MIT License. See `LICENSE` for details.

```

---

Let me know if youâ€™d like:
- A Colab notebook
- Web UI with Streamlit/Gradio
- Tokenizer and generation samples
- Hugging Face integration

I can help with all of those.
```
