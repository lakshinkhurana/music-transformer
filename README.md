# ğŸµ Music Transformer: Generating Music with Long-Term Structure

This repository is a reimplementation of the research paper:  
ğŸ“„ [Music Transformer: Generating Music with Long-Term Structure](https://arxiv.org/abs/1809.04281) by Huang et al., 2018.

> This model leverages the Transformer architecture with a novel **relative positional encoding** mechanism to generate coherent and structured musical sequences over long time spans.

---

## ğŸš€ Features

- Transformer-based architecture optimized for music generation.
- Support for **relative positional self-attention** for capturing long-term dependencies.
- Trained on MIDI-based datasets (e.g., MAESTRO, JSB Chorales).
- Support for both training and inference.
- Generates symbolic music in MIDI format.

---

## ğŸ§  Paper Contributions Summary

- Proposed a **relative attention mechanism** to overcome limitations of absolute position encodings for musical data.
- Demonstrated the model's capability to generate longer and more musically coherent sequences than baseline models.
- Introduced **event-based representation** of MIDI music for efficient tokenization.

---

## ğŸ—‚ï¸ Project Structure

```bash
music-transformer/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ midi/              # Input MIDI files (for training)
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ transformer.py     # Transformer model implementation
â”‚   â”œâ”€â”€ relative_attention.py  # Relative self-attention module
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ midi_utils.py      # MIDI parsing and encoding tools
â”‚   â””â”€â”€ training_utils.py  # Training/evaluation helpers
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml        # Hyperparameters and training configuration
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ generate.py            # Inference / Music generation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
````

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/music-transformer.git
cd music-transformer
pip install -r requirements.txt
```

**Main dependencies:**

* `torch`
* `pretty_midi`
* `numpy`
* `tqdm`
* `pytorch-lightning` *(optional)*

---

## ğŸ¼ Dataset

Use any of the following datasets (preprocessed into tokenized sequences):

* [MAESTRO Dataset (by Google)](https://magenta.tensorflow.org/datasets/maestro)
* [JSB Chorales Dataset](https://github.com/czhuang/JSB-Chorales-dataset)

To preprocess MIDI files:

```bash
python utils/midi_utils.py --input_dir data/midi --output_file data/encoded_data.pkl
```

---

## ğŸ‹ï¸ Training

Edit `configs/config.yaml` to set hyperparameters and paths.

Then run:

```bash
python train.py --config configs/config.yaml
```

---

## ğŸ§ Generation

To generate a MIDI file from a trained model:

```bash
python generate.py --checkpoint_path checkpoints/best_model.pth --output_dir generated/
```

This will generate a `.mid` file you can play using any MIDI player or DAW.

---

## ğŸ“Š Results

| Model Variant          | Perplexity â†“ | Coherence â†‘ | Notes                        |
| ---------------------- | ------------ | ----------- | ---------------------------- |
| Transformer (Baseline) | 1.78         | Medium      |                              |
| **Music Transformer**  | **1.63**     | **High**    | Achieves long-term structure |

Sample generations available in the `generated/` directory.

---

## ğŸ“Œ TODOs

* [x] Relative positional attention
* [x] MIDI tokenization and data loader
* [x] Training loop
* [x] Music generation script
* [ ] Beam search for generation
* [ ] Web interface (streamlit?)

---

## âœï¸ Citation

If you use this code or base your work on this implementation, please cite:

```bibtex
@article{huang2018musictransformer,
  title={Music Transformer: Generating Music with Long-Term Structure},
  author={Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Mihael and Eck, Douglas},
  journal={arXiv preprint arXiv:1809.04281},
  year={2018}
}
```

---

## ğŸ§  Credits

* Original authors: [Anna Huang et al.](https://magenta.tensorflow.org/music-transformer)
* Reimplementation by: [Your Name](https://github.com/yourusername)
* Inspired by: Magenta's official implementation and community forks

---

## ğŸ“¨ Contact

For queries, issues, or suggestions:
ğŸ“§ [your.email@example.com](mailto:your.email@example.com)
ğŸ¦ Twitter: [@yourhandle](https://twitter.com/yourhandle)

---

## ğŸª„ License

MIT License. See `LICENSE` for details.

```

---

Let me know if youâ€™d like:
- A Colab notebook
- Web UI with Streamlit/Gradio
- Tokenizer and generation samples
- Hugging Face integration

I can help with all of those.
```
